{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34eaa191-c264-4a4e-bfb6-cfcb3843ddc2",
   "metadata": {},
   "source": [
    "# Prilog: **Nesterovljev ubrzani gradijentni metod za minimizaciju (NAG)**  \n",
    "\n",
    "*Implementacija unutar Julia programskog jezika* \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f8052e",
   "metadata": {},
   "source": [
    "Za potrebe istoimenog seminarskog rada iz kursa _Numerički algoritmi_, prilažemo **implementaciju algoritma** uz primjenu na _poznatim problemima iz oblasti numeričkih metoda_, napisani u **Julia programskom jeziku**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f1b3f4",
   "metadata": {},
   "source": [
    "### 1. **Definicije**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55d9131",
   "metadata": {},
   "source": [
    " **Standardna formulacija Nesterovljeve metode**   \n",
    ">\n",
    "> $y_{k} = x_k + \\mu\\, v_k$  \n",
    "> $v_{k+1} = \\mu\\, v_k - \\alpha \\nabla f(y_k) \\quad \\{v_0 = \\vec{0}\\}$   \n",
    "> $x_{k+1} = x_k + v_{k+1}$  \n",
    "\n",
    "**Gdje je:**  \n",
    "\n",
    "> $v_{k}$ - Brzina spusta gradijenta u $k$-toj iteraciji _(Velocity)_  \n",
    "> $\\alpha$ - Stopa učenja _(Learning rate)_   \n",
    "> $\\mu$ - Koeficijent momentuma spusta  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e17aa53",
   "metadata": {},
   "source": [
    "Alternativno, u praktičnim primjenama, u literaturi se pojavljuju **i druge formulacije Nesterovljeve metode**. \n",
    "\n",
    "Među najpoznatijima su formulacije koje su predložili ***Sutskever et al.*** (2013), te ***Bengio*** (2012), koje prilagođavaju originalnu metodu kako bi je učinili pogodnijom za **primjenu u treniranju rekurentnih i dubokih neuronskih mreža _(RNN & DNN)_**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b546bc9",
   "metadata": {},
   "source": [
    "#### 1.1 **Sutskeverova formulacija metode** (Sutskever et al., 2013.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a3f308",
   "metadata": {},
   "source": [
    "Ključna ideja Sutskeverove modifikacije je **_'fazno'_ pomjeranje za pola iteracije**.\n",
    "\n",
    "Umjesto redoslijeda _**\"gradijent → momentum\"**_, granica iteracije se pomjera tako da se\n",
    "dobije redoslijed _**\"momentum → gradijent\"**_.  \n",
    "Rezultat je matematički ekvivalentna formulacija koja radi nad parametrima $\\phi$ umjesto\n",
    "$\\theta$, koja se pokazuje pogodnijom za potrebe _dubokog učenja_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f9325d",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "\\phi_{t+1} = \\phi_t + \\mu v_t - \\alpha \\nabla f(\\phi_t + \\mu v_t)\n",
    "$$\n",
    "\n",
    "> **Parametri $\\phi$ na kraju svake iteracije već su postavljeni na _look-ahead_ poziciju.**  \n",
    "> Tako, sljedeća iteracija automatski evaluira gradijent na ispravnom mjestu, \n",
    "> te **nema potrebe za eksplicitnim održavanjem dva odvojena vektora parametara** kao u\n",
    "> originalnoj Nesterovljevoj formulaciji, ostajući **u skladu sa standardnim gradijentnim metodama.**\n",
    "\n",
    "_Ova modifikacija je danas prisutna u mnogim softverskim okvirima _(framework-ima)_ za\n",
    "_duboko učenje_, kao što je **PyTorch**._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1f986e",
   "metadata": {},
   "source": [
    "#### 1.2 **Bengiova formulacija metode** (Bengio, 2012.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f244e65",
   "metadata": {},
   "source": [
    "Bengiova formulacija nastaje kao alternativa **Sutskeverovoj**, s ciljem lakše integracije \n",
    "u postojeće softverske okvire _(framework-e)_.  \n",
    " Ključna prednost je što **ne zahtijeva računanje gradijenta \n",
    "na nestandardnoj poziciji**, dovoljno je samo modificirati koeficijente u proračunu spusta, što je znatno jednostavnija izmjena u kodnoj bazi koja već koristi standardni gradijenti spust sa momentumom.\n",
    "\n",
    "\n",
    "Razvijanjem Sutskeverove formulacije i sređivanjem članova, dobija se matematički ekvivalentan izraz:\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t + \\mu_{t-1}\\mu_t b_t - (1+\\mu_t)\\alpha_t \\nabla f(\\theta_t)\n",
    "$$\n",
    "Gdje je, \n",
    "$$\n",
    "b_{t+1} = \\mu_t b_t - \\alpha_t \\nabla f(\\theta_t)\n",
    "$$\n",
    "\n",
    "> **Bengiova formulacija** pokazuje da je _look-ahead_ perspektiva samo jedan način gledanja, te da se NAG ekvivalentno \n",
    "> može posmatrati kao momentum s **korigiranim koeficijentima**, gdje metoda primjenjuje **veći efektivni korak gradijenata** i **manji efektivni korak momentuma**.  što je pogodnije za analizu stabilnosti treniranja _RNN_.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbd371f",
   "metadata": {},
   "source": [
    "### 2. **Formulacija metode u programskom kodu**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0056bdee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nag (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------------------------------------------------\n",
    "# Nesterovljeva metoda za minimizaciju - Standardna formulacija\n",
    "# -------------------------------------------------\n",
    "\n",
    "function nag(x0, func, gradient;\n",
    "             learning_rate = 0.001,\n",
    "             momentum_coeff = 0.9,\n",
    "             max_iter = 5000,\n",
    "             eps = 1e-7)\n",
    "\n",
    "    v = zeros(length(x0)) # Brzina spusta\n",
    "    x = copy(x0) # Trenutna pozicija\n",
    "\n",
    "    path = [copy(x)] # Za potrebe grafičkog prikaza kretanja metode\n",
    "    for i in 1:max_iter\n",
    "        look_ahead = x .+ momentum_coeff .* v\n",
    "        grad_x = gradient(look_ahead)\n",
    "        if norm(grad_x) < eps\n",
    "            break\n",
    "        end\n",
    "        v = momentum_coeff .* v .- learning_rate .* grad_x\n",
    "        x = x .+ v\n",
    "\n",
    "       push!(path, copy(x))\n",
    "    end\n",
    "    push!(path, copy(x))\n",
    "    return (x=x,f=func(x),path=path)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbdefa0",
   "metadata": {},
   "source": [
    "> ***Sutskever formulacija Nesterovljeve metode***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29f946fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nag_sutskever (generic function with 1 method)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -------------------------------------------------\n",
    "# Nesterovljeva metode za minimizaciju - Sutskeverova formulacija\n",
    "# -------------------------------------------------\n",
    "\n",
    "\n",
    "function nag_sutskever(x0, func, gradient;\n",
    "                       learning_rate  = 0.001,\n",
    "                       momentum_coeff = 0.9,\n",
    "                       max_iter       = 5000,\n",
    "                       eps            = 1e-7)\n",
    "\n",
    "    v    = zeros(length(x0))\n",
    "    x    = copy(x0)\n",
    "    path = [copy(x)]\n",
    "\n",
    "    for i in 1:max_iter\n",
    "        v_prev = copy(v)\n",
    "        v      = momentum_coeff .* v .- learning_rate .* gradient(x .+ momentum_coeff .* v)\n",
    "        x      = x .+ v\n",
    "        if norm(grad) < eps \n",
    "            break\n",
    "        end\n",
    "        push!(path, copy(x))\n",
    "    end\n",
    "    push!(path, copy(x))\n",
    "    return (x=x, f=func(x), path=path)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20377a1",
   "metadata": {},
   "source": [
    "> ***Bengio formulacija Nesterovljeve metode***\n",
    "\n",
    "Ovdje ćemo našu implementaciju Bengiove formulacije prikazati sa blagim odstupanjem od teoretske u jednoj ključnoj tački:  \n",
    "> $\\mu_{t-1}\\mu_t$ se aproksimira kao $\\mu^2$, što je ispravno samo kada je $\\mu = const.$ kroz sve iteracije.   \n",
    "\n",
    "U teoretskoj formulaciji $\\mu_t$ može biti raspoređen\n",
    "_(scheduled)_ po iteracijama, što bi zahtijevalo eksplicitno praćenje $\\mu_{t-1}$.  \n",
    "\n",
    "Dodatno, vektor $b_t$ iz teoretske formulacije predstavlja razliku parametara \n",
    "$\\phi_{t+1} - \\phi_t$, dok implementacija koristi $v_t$ koji se ažurira standardnom\n",
    "momentum formulom   \n",
    "$v = \\mu v - \\alpha \\nabla f(\\theta)$ što je ekvivalentno\n",
    "samo pod pretpostavkom $\\mu = const.$\n",
    "\n",
    "_Za potrebe ovog rada, gdje je $\\mu$ fiksan, ova aproksimacija ne uvodi grešku. Slučajevi gdje je zaista nezanemarljiva greška posljedicom ove aproksimacije su češto veoma specifični, te van sklopa ovog rada, a služe boljoj vizualizaciji ekvivalencije sa originalnom formulacijom._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c63c0e57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nag_bengio (generic function with 1 method)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function nag_bengio(x0, func, gradient;\n",
    "                    learning_rate  = 0.001,\n",
    "                    momentum_coeff = 0.9,\n",
    "                    max_iter       = 5000,\n",
    "                    eps            = 1e-7)\n",
    "\n",
    "    v    = zeros(length(x0))\n",
    "    x    = copy(x0)\n",
    "    path = [copy(x)]\n",
    "\n",
    "    for i in 1:max_iter\n",
    "        grad = gradient(x)\n",
    "        if norm(grad) < eps \n",
    "            break\n",
    "        end\n",
    "\n",
    "        if i == 1\n",
    "            x = x .- learning_rate .* grad\n",
    "        else\n",
    "            x = x .+ momentum_coeff^2 .* v .- (1 + momentum_coeff) .* learning_rate .* grad\n",
    "            v = momentum_coeff .* v .- learning_rate .* grad\n",
    "        end\n",
    "\n",
    "        push!(path, copy(x))\n",
    "    end\n",
    "    push!(path, copy(x))\n",
    "    return (x=x, f=func(x), path=path)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619757f7",
   "metadata": {},
   "source": [
    "Za potrebe ove aproksimacije, potrebno je obratiti pažnju na dodatni problem u prvoj iteraciji, $i=1$: \n",
    "\n",
    ">U prvoj iteraciji radimo čisti gradijentni spust (bez momentuma).  \n",
    "> **Razlog:** $\\mu_{t-1}\\mu_t = 0 \\cdot \\mu = 0$, pa momentum član ispada.\n",
    ">\n",
    ">Ako bismo ažurirali $v$ na prvoj iteraciji, narušili bismo pretpostavku\n",
    "$\\mu_t \\cdot \\mu_{t-1} \\approx \\mu^2$ za iteraciju $t=2$, jer bi $\\mu_1 = 0 \\neq \\mu$.\n",
    "\n",
    "*Rješenje ove anomalije preuzimamo od James Melville-a, autora paketa \n",
    "<a href=\"https://github.com/jlmelville/mize\">mize</a>\n",
    " za algoritme numeričke minimizacije funkcija u programskom jeziku R, gdje je više o ovom pisao u svom članku (Melville, 2016).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd197c4",
   "metadata": {},
   "source": [
    "### 3. ***Primjer i vizualizacija primjene metode***  \n",
    " *Minimizacija Rosenbrockove funkcije* \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a371681c",
   "metadata": {},
   "source": [
    "#### 3.1 **Rosenbrockova funkcija**\n",
    "\n",
    "**Rosenbrockova funkcija** _(Rosenbrock, 1960)_ je klasična testna funkcija u oblasti\n",
    "numeričke optimizacije, definisana kao:\n",
    "\n",
    "$$\n",
    "f(x, y) = (a - x)^2 + b(y - x^2)^2\n",
    "$$\n",
    "\n",
    "Globalni minimum se nalazi u tački $(x, y) = (a, a^2)$, tako da za učestale parametre **$a = 1,\\ b = 100, f(1, 1) = 0$**.\n",
    "\n",
    "Funkcija je poznata po svojoj **uskoj, zakrivljenoj dolini** koja vodi prema minimumu —\n",
    "dolina je lako pronađena, ali konvergencija unutar nje je spora zbog zakrivljenosti\n",
    "i loše uvjetovanosti _(ill-conditioning)_. Upravo zbog toga se koristi kao standardni\n",
    "_benchmark_ za testiranje metoda minimizacije, posebno gradijentnih metoda s momentumom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac130ef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"c:\\\\Users\\\\musta\\\\Desktop\\\\Workspace\\\\Fakultet\\\\NA\\\\nesterov-accelerated-gradient\\\\src\\\\rosenbrock_contour.png\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rosenbrock(x) = (1 - x[1])^2 + 100 * (x[2] - x[1]^2)^2\n",
    "\n",
    "function rosenbrock_grad(x)\n",
    "    dx1 = -2*(1 - x[1]) - 400*x[1]*(x[2] - x[1]^2)\n",
    "    dx2 =  200*(x[2] - x[1]^2)\n",
    "    [dx1, dx2]\n",
    "end\n",
    "\n",
    "contourf(-2:0.01:2, -1:0.01:3, \n",
    "    [rosenbrock([x,y]) for y in -1:0.01:3, x in -2:0.01:2];\n",
    "    levels=50, color=:viridis,title=\"Rosenbrockova funkcija\", xlabel=\"x\", ylabel=\"y\")\n",
    "\n",
    "scatter!([1.0], [1.0];\n",
    "    marker            = :star5,\n",
    "    markersize        = 11,\n",
    "    color             = :white,\n",
    "    markerstrokecolor = :black,\n",
    "    label             = \"Minimum \\$f(1,1) = 0\\$\")\n",
    "\n",
    "savefig(\"rosenbrock_contour.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb6e2d7",
   "metadata": {},
   "source": [
    "Primjenjimo sada sve tri prikazane formulacije metode na funkciju."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb39591e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAG (classic)   - iteracije: 3576,   x*: [1.0, 1.0],   f*: 1.259e-14\n",
      "NAG (Sutskever) - iteracije: 1153, x*: [0.99744, 0.99488], f*: 6.556e-6\n",
      "NAG (Bengio)    - iteracije: 3617, x*: [1.0, 1.0], f*: 1.247e-14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"c:\\\\Users\\\\musta\\\\Desktop\\\\Workspace\\\\Fakultet\\\\NA\\\\nesterov-accelerated-gradient\\\\src\\\\rosenbrock_nag_paths.png\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Plots, LinearAlgebra\n",
    "x0       = [-1.5, 1.5]\n",
    "result   = nag(x0, rosenbrock, rosenbrock_grad)\n",
    "result_s = nag_sutskever(x0, rosenbrock, rosenbrock_grad)\n",
    "result_b = nag_bengio(x0, rosenbrock, rosenbrock_grad)\n",
    "\n",
    "println(\"NAG (classic)   - iteracije: $(length(result.path)-1),   x*: $(round.(result.x, digits=5)),   f*: $(round(result.f, sigdigits=4))\")\n",
    "println(\"NAG (Sutskever) - iteracije: $(length(result_s.path)-1), x*: $(round.(result_s.x, digits=5)), f*: $(round(result_s.f, sigdigits=4))\")\n",
    "println(\"NAG (Bengio)    - iteracije: $(length(result_b.path)-1), x*: $(round.(result_b.x, digits=5)), f*: $(round(result_b.f, sigdigits=4))\")\n",
    "\n",
    "xs = -2:0.02:2\n",
    "ys = -0.5:0.02:3.0\n",
    "Z  = [rosenbrock([x, y]) for y in ys, x in xs]\n",
    "\n",
    "path_x  = [p[1] for p in result.path];   path_y  = [p[2] for p in result.path]\n",
    "path_xs = [p[1] for p in result_s.path]; path_ys = [p[2] for p in result_s.path]\n",
    "path_xb = [p[1] for p in result_b.path]; path_yb = [p[2] for p in result_b.path]\n",
    "\n",
    "contourf(xs, ys, Z; levels=40, color=:viridis, linewidth=0,\n",
    "    xlabel=\"X-osa\", ylabel=\"Y-osa\",\n",
    "    title=\"Minimizacija Rosenbrockove funkcije Nesterovljevom metodom\",\n",
    "    size=(820, 620))\n",
    "\n",
    "plot!(path_xb, path_yb; color=:cyan,  linewidth=2.5, label=\"NAG (Bengio)\",    linestyle=:solid)\n",
    "plot!(path_xs, path_ys; color=:red,   linewidth=1.5, label=\"NAG (Sutskever)\", linestyle=:dash)\n",
    "plot!(path_x,  path_y;  color=:white, linewidth=1.0, label=\"NAG (classic)\",   linestyle=:dot)\n",
    "\n",
    "scatter!([x0[1]], [x0[2]]; color=:white,  markersize=8, marker=:circle, label=\"Start\")\n",
    "scatter!([path_x[end]],  [path_y[end]];  color=:white, markersize=8, label=\"Converged NAG (classic)\")\n",
    "scatter!([path_xs[end]], [path_ys[end]]; color=:red,   markersize=8, label=\"Converged NAG (Sutskever)\")\n",
    "scatter!([path_xb[end]], [path_yb[end]]; color=:cyan,  markersize=8, label=\"Converged NAG (Bengio)\")\n",
    "scatter!([1.0], [1.0]; color=:yellow, markersize=11, marker=:star5, label=\"Rosenbrock minimum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfad649",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09cdd19a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Literatura**\n",
    "\n",
    "- Sutskever, I., Martens, J., Dahl, G., Hinton, G. (2013). *On the importance of \n",
    "  initialization and momentum in deep learning*. ICML, PMLR 28.\n",
    "\n",
    "- Bengio, Y. (2012). *Practical Recommendations for Gradient-Based Training of \n",
    "  Deep Architectures*. Neural Networks: Tricks of the Trade, Springer.\n",
    "\n",
    "- Melville, J. (2016). *Nesterov Accelerated Gradient and Momentum*. \n",
    "Preuzeto sa https://jlmelville.github.io/mize/nesterov.html\n",
    "\n",
    "- Rosenbrock, H. H. (1960). An automatic method for finding the greatest or least value \n",
    "of a function. *The Computer Journal*, 3(3), 175–184. \n",
    "https://doi.org/10.1093/comjnl/3.3.175"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.12",
   "language": "julia",
   "name": "julia-1.12"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
