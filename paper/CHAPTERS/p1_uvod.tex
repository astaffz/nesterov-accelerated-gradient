\chapter{Uvod}

%U skladu sa dobrom istraÅ¾ivaÄkom praksom, uvodno poglavlje rada prvog ciklusa studija bi trebao sadrÅ¾avati bar sljedeÄ‡e elemente:
%\begin{itemize}
%\item {obrazloÅ¾enje teme,}
%\item {opis strukture rada.}
%\end{itemize}

%U narednom tekstu Ä‡e detaljnije biti obrazloÅ¾ena svaka od taÄaka.

\section{ObrazloÅ¾enje teme}

%Å ta je minimizacija?

%Å ta je gradijent?

%Å ta je klasiÄni momentum?

%Kako NAG pravi napredak u odnosu na klasiÄni momentum?

%Koje ima NAG interpretacija?

%Kakva je brzina konvergencije?




Problem minimizacije funkcija predstavlja jedan od najvaÅ¾nijih problema primijenjene matematike. U najopÅ¡tijem obliku, cilj je odrediti taÄku
\(\mathbf{x}  \in \mathbb{R} ^n\) u kojoj funkcija \(f(\mathbf{x})\) dostiÅ¾e minimalnu vrijednost. Ovakvi problemi javljaju se u brojnim oblastima primijenjene matematike, fizike, ekonomije i savremenog maÅ¡inskog uÄenja.


Jedna od osnovnih metoda za rjeÅ¡avanje problema minimizacije diferencijabilnih funkcija jeste gradijentni metod. Gradijent funkcije predstavlja vektor parcijalnih izvoda i pokazuje pravac najveÄ‡eg rasta funkcije, pa se minimizacija postiÅ¾e kretanjem u suprotnom smjeru gradijenta. Iako je metoda jednostavna i Å¡iroko primjenjiva, njena konvergencija moÅ¾e biti spora, naroÄito kod loÅ¡e uslovljenih problema.

Radi poboljÅ¡anja performansi razvijene su metode koje koriste dodatne informacije iz prethodnih iteracija. Jedan od prvih pristupa je metod momentuma, koji uvodi â€œinercijuâ€ u proces optimizacije i na taj naÄin ublaÅ¾ava oscilacije i ubrzava kretanje kroz ravnije dijelove funkcije.

Nesterov ubrzani gradijent metod predstavlja unapreÄ‘enje klasiÄnog momentum pristupa. Za razliku od standardnog momentuma, kod Nesterovog metoda gradijent se raÄuna u unaprijed predviÄ‘enoj taÄki, Äime se dobija efikasniji i stabilniji postupak. Ova modifikacija dovodi do znaÄajnog teorijskog poboljÅ¡anja brzine konvergencije.

Cilj ovog rada je da se prikaÅ¾e teorijska osnova Nesterovog ubrzanog gradijentnog metoda, objasne njegove interpretacije, analiziraju osobine konvergencije, te implementira algoritam i ispita njegovo ponaÅ¡anje na Rosenbrockovoj funkciji.


%Postupci minimizacije se mogu gledati kao jedno stablo. Na prvom stepenu moÅ¾emo izabrati tip minimizacije i izabraÄ‡emo gradijentski. od svih moguÄ‡ih gradijentskih, izabraÄ‡emo Nesterovljev ubrzani gradijentni metod.

%In a groundbreaking paper in 1983, Nesterov, Y. [Nes83] showed that a simple variant of gradient de- scentâ€”called accelerated gradient descent and applicable to any ð¿-smooth convex functionâ€”produces iterates with optimality gap ð‘“(ð‘¥ð‘¡) âˆ’ ð‘“â‹† of order 1/ð‘¡2, as opposed to the 1/ð‘¡ rate seen in the previous lecture. The intuition behind accelerated gradient descent is notoriously hard to grasp. The original proof, rife with algebraic manipulations, is notoriously elusive and has led several authors to investi- gate what principles make acceleration possible at a deep level, hoping to generalize the fundamental principles beyond just gradient descent. These efforts include at least the following directions




% U ovoj sekciji autor je duÅ¾an da obrazloÅ¾i koja tema ili problem Ä‡e biti analizirani ili istraÅ¾ivani, te zbog Äega je upravo ova tema pogodna i bitna za istraÅ¾ivanje. Pohvalno je napraviti pregled literature sa odgovarajuÄ‡im referenciranjem na istu.

\section{Struktura rada}
%U ovoj sekciji je najbolje dati po jedan paragraf o svakom poglavlju iz rada. Potencirajte koji su glavni doprinosi svakog poglavlja, te kako su poglavlja medjusobno povezana.


Rad je organizovan u 4 poglavlja. U prvom poglavlju se opisuje struktura rada i teoretska osnova potrebna za razumjevanje Nesterovog ubrzanog gradijenta, kao i zaÅ¡to i kada bi se trebao koristiti.

U drugom poglavlju Ä‡e biti prikazane tri tipa implementacija Nesterovog ubrzanog gradijenta, to jest standardna varijanta, Sutskeverova modifikacija i Bengiova formulacija metode. Zatim Ä‡e biti prikazan jedan praktiÄan primjer rada implementiranih funkcija i usporediÄ‡e se njihovi rezultati.

TreÄ‡e poglavlje Ä‡e govoriti o primjenama algoritma u praksi, kao Å¡to su u dubokom uÄenju, modernim softverskim okvirima, optimizatorima te obradi signala i kompresovanog opaÅ¾anja.


U Äetvrtom smo oformili zakljuÄak.


\section{Teoretske osnove}

\subsection{Metod momentuma}

Jedno od prvih poboljÅ¡anja klasiÄnog gradijentnog metoda predstavlja metod momentuma, koji je uveo Boris Polyak. Osnovna ideja ovog pristupa jeste da se pri aÅ¾uriranju iteracija ne koristi samo trenutna vrijednost gradijenta, veÄ‡ i informacija o prethodnom kretanju algoritma.

\begin{equation}
    v_{t+1} = \mu v_t - \varepsilon \nabla  f(\theta _t)
\end{equation}

\begin{equation}
    \theta _{t+1} = \theta _t + v_{t+1}
\end{equation}

Gdje \(\varepsilon > 0 \) predstavlja parametar koraka, \(\mu \epsilon [0,1] \) parametar momentuma, a \( \nabla  f(\theta _t) \) gradijent.\cite{sutskever2013}

\subsection{Nesterov ubrzani gradijentni metod}

Metod Nesterovog ubrzaniog gradijenta je metod koji izgleda kao metod momentuma, ali nije u potpunosti isti.\cite{melville2016}

\begin{equation}
    v_{t+1} = \mu v_t - \varepsilon \nabla  f(\theta _t +\mu v_t)
\end{equation}

\begin{equation}
    \theta _{t+1} = \theta _t + v_{t+1}
\end{equation}

Razlika je u argumentu gradijenta \(+\mu v_t\), omoguÄ‡uje metodu da brÅ¾e popravi putanju kretanja od metod momentuma. \cite{sutskever2013}

% \subsection{Formalna analiza konvergencije}




